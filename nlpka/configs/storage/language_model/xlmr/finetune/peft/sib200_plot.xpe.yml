mode: test # train | finetune  | test

task:
    id: null
    category: text_classification # null | text_classification | token_classification
    name: topic # Topic Detection
    peft:
        # 
        ## PeftConfig(PeftConfigMixin): This is the base configuration class to store the configuration of a [`PeftModel`].
        peft_type: P_TUNING # (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.
        task_type: SEQ_CLS # (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.
        # inference_mode: False # (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.
        # 
        ## PromptLearningConfig(PeftConfig): This is the base configuration class to store the configuration of [`PrefixTuning`], [`PromptEncoder`], or [`PromptTuning`].
        num_virtual_tokens: 20 # (`int`): The number of virtual tokens to use.
        # token_dim: # (`int`): The hidden embedding dimension of the base transformer model.
        # num_transformer_submodules: # (`int`): The number of transformer submodules in the base transformer model.
        # num_attention_heads: # (`int`): The number of attention heads in the base transformer model.
        # num_layers: # (`int`): The number of layers in the base transformer model.
        #
        ## PromptEncoderConfig(PromptLearningConfig): This is the configuration class to store the configuration of a [`PromptEncoder`].
        encoder_reparameterization_type: MLP # (Union[[`PromptEncoderReparameterizationType`], `str`]): The type of reparameterization to use. (MLP, LSTM)
        encoder_hidden_size: 256 # (`int`): The hidden size of the prompt encoder.
        encoder_num_layers: 2 # (`int`): The number of layers of the prompt encoder.
        encoder_dropout: 0.1 # (`float`): The dropout probability of the prompt encoder.
        #    
        ## CrossPromptEncoderConfig(PromptEncoderConfig): This is the configuration class to store the configuration of a [`CrossPromptEncoder`].
        encoder_input_size: null
        encoder_init_state_dict_path: /fscratch/bmikaberidze/XPE/nlpka/models/storage/xlmr/FacebookAI|xlm-roberta-large/topic/ad56d282-e924-4026-b8fb-691721266098_FacebookAI|xlm-roberta-large_2021036_1_10_32_text_classification|topic|sib200_hf|source_xlmr_seen|tokenized|FacebookAI|xlm-roberta-large_source_xlmr_seen
        encoder_freeze: false
        encoder_embedding_freeze: false
        encoder_embedding_init_type: hf_default # hf_default | xavier_uniform | xavier_normal
        encoder_embedding_normalize: null # null | unit | clip
        encoder_embedding_normalize_max_norm: null
        # new xpe config >>
        encoder_ratio: 1
        # old ape config >>
        # encoder_embedding_type: FULLY_SHARED # FULLY_SHARED | PARTIALLY_DEDICATED | PARTIALLY_DEDICATED_DIM
        # encoder_embedding_dedicated_ratio: 0
        # encoder_embedding_neutral_dedication: 1
        # encoder_embedding_skip_for_dedicated: true
        # encoder_embedding_dedicated_init: null # ZEROS | RANDOM | AVERAGE | SPECIFIC
        # encoder_embedding_dedicated_init_key: -1 # source task key - only used if encoder_embedding_dedicated_init is SPECIFIC (for zero-shot evaluation set -1 that will choose task-agnostic neutral embedding (if it is trained at all))
       

model:
    architecture: xlmr # bert | roberta | xlnet | electra | xlm
    pretrained:
        name: FacebookAI|xlm-roberta-large
        source: local # null | local | huggingface
        checkpoint: null
        # adapter:
        #     name: null
        #     source: local # null | local | huggingface
        #     checkpoint: null
        #     uuid4: 1b29aa40-1bb9-4dd1-adbb-c1a208c2b724

tokenizer:
    source: huggingface # local | huggingface
    name: FacebookAI/xlm-roberta-large
    type: null # null | wordpiece | sentpiece
    algorithm: null
    adapt_to_lm: false

ds:
    descriptive_name: null
    category: benchmarks # benchmarks | corpora | collections | raw
    dirs: text_classification/topic/sib200_hf/ace_Arab # sa/imdb_reviews_ka | sa/google_maps_reviews
    name: null
    type: huggingface_saved # text | csv | huggingface_saved | huggingface
    comes_with_splits: 
        train: true
        test: true
        validation: true
    input:
        key: inputs
        standardize_key: false
    label:
        key: labels
        number: 7
        names:
        - science/technology
        - travel
        - politics
        - sports
        - health
        - entertainment
        - geography
        standardize_key: false
    task_id:
        key: task_ids
        standardize_key: false
    preproc_rules: null

eval:
    before_training: false
    before_training_on_test: false
    during_training: null
    after_training: false
    after_training_on_test: false
    per_task: null
    metric_groups:
    - metrics: 
        - accuracy
    # Token Classification
    flatten: false
    filter_padded: false
    label_id_to_name: false
    filter_by_prefixes: false
    # Text To Text
    decode: false
    label_name_strip_lower: false
    label_name_to_float: false
    label_name_to_id: false
    # Others
    verify_labels_match: false
    calc_confusion_matrix: false
    prediction_axis: -1
    downstream_tasks: false

test:
    run: true
    zero_shot: true
    zero_shot_only: true
    save_predictions: false
    report_to_wandb: true

data_collator:
    max_length: 164
    padding: true # true | max_length | false
    pad_to_multiple_of: 4
    return_tensors: pt

custom_training_args:
    # Sampling
    train_force_sequential: false
    eval_force_sequential: true
    test_force_sequential: true
    # Early Stopping
    early_stopping_after: 0.5 # Donâ€™t allow early stopping before X% of total training steps.
    early_stopping_patience: 30
    early_stopping_threshold: 0.0
    # Final Model Saving
    save_final_model: true
    keep_only_final_model: true
    # Random Task Exclusion
    random_task_exclusion: false
    # Do not remove in _remove_unused_columns
    usable_columns:
    - task_ids
    optimizer_grouped_parameters:
    - param_name_parts:
        - dedicated_embeddings
      lr: 5e-5
      weight_decay: 0.01
    # - param_name_parts:
    #     - shared_embedding
    #   lr: 1e-6
    #   weight_decay: 0.01

training_args:
    # Epochs
    num_train_epochs: 10
    # max_steps: 6000

    # Batch Size
    per_device_train_batch_size: 32
    gradient_accumulation_steps: 1
    per_device_eval_batch_size: 256
    eval_accumulation_steps: 1

    # Logging
    logging_strategy: steps # steps
    logging_first_step: false
    logging_steps: 50
    report_to: wandb # wandb | none

    # Evaluation
    eval_strategy: epoch  # "steps" or "epoch" for evaluation at the end of each epoch
    eval_steps: null  # How often to run evaluation during training

    # Saving
    save_strategy: epoch # no / steps / epoch / best
    save_steps: null
    save_total_limit: 1
    save_safetensors: true
    overwrite_output_dir: true
    push_to_hub: false

    # Loading Best Model
    load_best_model_at_end: true # the best model (in terms of evaluation metric) is loaded at the end of training    
    metric_for_best_model: accuracy # specify the metric to determine the 'best' model
    greater_is_better: true # whether a larger metric value is better

    # Early Stopping
    # early_stopping_patience: 3 # default: 3 - Number of epochs to wait before stopping the training if the validation loss does not improve
    # early_stopping_threshold: 0.001 # default: 0.0 - Minimum change in the monitored quantity to qualify as an improvement
    # early_stopping_metric: accuracy # default: loss - Metric to monitor for early stopping

    # Optimizer
    learning_rate: 5e-5
    lr_scheduler_type: cosine_with_restarts
    lr_scheduler_kwargs: 
        num_cycles: 2
    warmup_ratio: 0.1
    weight_decay: 0.01
    max_grad_norm: 1.0
  
    # Dataloader
    # dataloader_num_workers: 4 # default: 4 - Number of workers for the dataloader
    # dataloader_pin_memory: true # default: True - Whether to pin memory for the dataloader
    # dataloader_persistent_workers: true # default: False - Whether to use persistent workers for the dataloader

    # Grouping
    group_by_length: true # Whether to group samples by length or not while batching
    length_column_name: length

    # Others
    # predict_with_generate: True  # Important for generation tasks
    full_determinism: false
    # remove_unused_columns: false  # KEEP all columns    
    bf16: true
    # fp16_opt_level: O2
    fp16: false

cuda:
    empty_cache_steps: null

# While loading configuration 
# this parameters are added to the environment variables
env:

    # CUDA_LAUNCH_BLOCKING: '1'
    PYTORCH_CUDA_ALLOC_CONF: null # max_split_size_mb:128

    # huggingface/tokenizers: The current process just got forked, after parallelism has already been used. 
    # Disabling parallelism to avoid deadlocks...
    # To disable this warning, you can either:
    #         - Avoid using `tokenizers` before the fork if possible
    #         - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
    TOKENIZERS_PARALLELISM: 'false' # 'true' | 'false'
